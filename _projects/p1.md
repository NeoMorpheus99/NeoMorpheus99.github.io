---
layout: "single"
title: "Action Recognition model for Sign Language Translation"
excerpt: "<img src='/images/p1/Picture10.jpg'><br/><br> AI/ML and computer vision related Project"
collection: projects
---

<div style="text-align: center;">
  <img title="a title" alt="Alt text" src="/images/p1/Picture10.jpg" width = "50%" height ="50%">
</div><br>

# Description

This project focuses on developing a sign language translation (SLT) tool using computer vision and machine learning techniques to facilitate communication for individuals with hearing impairments. By leveraging the MediaPipe library, the system detects and tracks key points in human gestures, storing them as NumPy arrays for further processing. A machine learning model, specifically Long Short-Term Memory (LSTM), is implemented to recognize and accurately translate these gestures. The trained model can then be used as an assistive communication tool for individuals who are unable to speak, enhancing accessibility and inclusivity.

### Team and responsibiltiy:

- **Norbu Tshering Lepcha** : sample collection, data preparation, model training, and testing during the coding phase. In order to aid in the model's training.
- **Sibonakaliso K. Mdlovu** : reasearched on why we should use Long Short-Term Memory (LSTM) networks for creating the model, aided in collecting the research paper as well as read and extracting informatiom from the paper, to guide the progress of the model creation.
- **Ankit Yadav** : implement the feature extraction process using the MediaPipe Holistic module and research on project planning.
- **Aditya Mani** : implement the feature extraction process using the MediaPipe Holistic module and research on standards to adopt for research

# Challenges Faced and solution

Building a sign language translation (SLT) model comes with several challenges:

1.  **Lack of Training Data** – Sign language datasets are limited, especially for specific languages or dialects. Collecting and annotating high-quality gesture data is time-consuming and requires collaboration with sign language experts.

    **Solution** : I created a dataset by setting up a folder structure with subfolders for each action to be recognized and translated. The selected words for this model were ‘Namaste,’ ‘Hello,’ ‘Great,’ ‘Bye,’ and ‘Thank You.’ Using a webcam, I captured gestures for each word, storing every 30 frames as a NumPy array. For each word, I recorded 30 videos, with each video consisting of 30 frames, effectively making each video a sequence of 30 images.

2.  **Complexity of LSTM Understanding** – LSTM models are powerful for sequential data processing, but understanding their internal workings, hyperparameter tuning, and ensuring effective learning can be challenging, especially for beginners.

    **Solution** : Studied LSTM theory in-depth, including how it handles sequential data and long-term dependencies.

3.  **Variability in Sign Language** – Different people sign differently due to variations in speed, style, and regional differences, making it difficult to create a generalized model that accurately interprets gestures across users.

    **Solution** : Narrowed down to American Sign Language for this particular project

4.  **Real-time Gesture Recognition** – Ensuring smooth, real-time detection and translation of gestures requires efficient processing, minimizing delays while maintaining accuracy.

    **Solution** : Optimized the MediaPipe framework to extract key points efficiently

5.  **Feature Extraction from MediaPipe** – While MediaPipe helps in detecting key points, translating these into meaningful representations for the LSTM model requires careful preprocessing and feature engineering.

    **Solution** :Converted extracted key points into structured NumPy arrays for better model training

# Result

The model achieved an impressive accuracy of nearly 99%, effectively learning and interpreting user actions with high precision. A key observation from this research was that the quality of the dataset significantly influenced model performance, impacting the accuracy of predictions. The optimal dataset size was determined to be 30 videos per word, with each video containing 30 frames, and the model trained for 2000 epochs. Increasing the number of frames introduced additional data points, which often acted as noise, leading to a decline in model performance due to overfitting or unnecessary complexity.

[View Report Here](/files/actionsdetection.pdf)
